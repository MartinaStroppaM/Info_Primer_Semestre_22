{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pasos Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm \n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(path_al_archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSPECCIONAR: ver que columnas tiene, que tipos de datos, cuantos nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "# Averigua la distribucion general de la tabla y nos dice los nombres de las columnas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() \n",
    "#Muestra cuantos datos nulos hay por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(datos.isnull(), cmap = \"viridis\") \n",
    "#Grafico de datos nulos\n",
    "#Hace la inspección de los datos faltantes de forma visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARACTERIZAR: DESCRIBE, saber cual es la media, mínima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "#Análisis estadístico básico del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos calcular la frecuencia de aparición de cada categoría/datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['columna'].value_counts()\n",
    "#Cuenta cuántas veces del total de filas apparece cada una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selección de columnas numéricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = list(datos.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(columnas[:2]\n",
    "columnas\n",
    "#Miramos el codigo anterior, buscamos las columnas no numericas y las eliminamos de COLUMNAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificacion_de_medias(lista):\n",
    "    test = {}\n",
    "    for columna in lista:\n",
    "        w , p = stats.shapiro(datos[columna].dropna())\n",
    "        test[columna] = p\n",
    "    return test\n",
    "# Realiza el test de normal para cada una de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verificacion_de_medias(columnas) \n",
    "#TIENE QUE DAR 0.05 O MAS PARA QUE ESTE NORMALIZADA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Limpiar el dataframe de las 3 posibles anomalias**\n",
    "\n",
    "\n",
    "PASOS EN EL CASO QUE SEA NECESARIO REEMPLAZAR LOS NAN\n",
    "\n",
    "1) Saco los duplicados\n",
    "\n",
    "2) Outliers\n",
    "\n",
    "3) Recién ahí puedo calcular la media “limpia” si necesitara rellenar los NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTLIERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMO CALCULAR LOS OUTLIERS!!\n",
    "\n",
    "Cs1 = datos[\"Actividad (en hs)\"].quantile(0.95)\n",
    "Ci1 = datos[\"Actividad (en hs)\"].quantile(0.05)\n",
    "Cs2 = datos[\"Acceso a Instagram (en hs)\"].quantile(0.95)\n",
    "Ci2 = datos[\"Acceso a Instagram (en hs)\"].quantile(0.05)\n",
    "#Averiguo el valor más chico y más grande de cada columna (los cuantiles)\n",
    "\n",
    "datos1 = datos[(datos[\"Actividad (en hs)\"] >= Ci1) & (datos[\"Actividad (en hs)\"] <= Cs1) & (datos[\"Acceso a Instagram (en hs)\"] >= Ci2) & (datos[\"Acceso a Instagram (en hs)\"] <= Cs2)]\n",
    "datos1\n",
    "# En la última fórmula quedate con los datos que se encuentren entre los cuantiles que calculamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREAMOS UN NUEVO DATAFRAME (variable datos1) SIN LOS OUTLIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATOS NULOS**\n",
    "\n",
    "- Eliminar los valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos2 = datos1.dropna().reset_index(drop=True) \n",
    "datos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos1.dropna(inplace=True).reset_index(drop=True) \n",
    "#si quiero eliminar los datos nulos directamente sobre datos 1 uso este metodo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rellenar los valores nulos con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df[\"columna_con_faltantes\"].mean(), inplace= True)\n",
    "# Columna_con_faltantes --> columna con muchos datos nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELIMINAR DATOS DUPLICADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos3 = datos2.drop_duplicates().reset_index(drop=True)\n",
    "datos3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos1.drop_duplicates(inplace=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(datos3)\n",
    "#luego de toda la limpieza deberia empezar a ver una correlacion entre grupos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NORMALIZACIÓN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "datos_escalado = scaler.fit_transform(datos3[columnas])\n",
    "#Es importa ver que selecciona COLUMNAS --> es decir las columnas numericas que seleccione antes!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CLUSTERING**\n",
    "\n",
    "**Cálculo de inercias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inercias_por_k(df_escalado):\n",
    "  inercias = {}\n",
    "  for i in range(1,11):\n",
    "      kmeans = KMeans(n_clusters = i, init=\"random\", n_init=10, max_iter=300, random_state=123457)\n",
    "      kmeans.fit(df_escalado)\n",
    "      inercias[i] = kmeans.inertia_\n",
    "  return inercias\n",
    "\n",
    "#Me da por cada grupo el valor de inercia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inercias = inercias_por_k(datos_escalado)\n",
    "inercias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inercias = pd.DataFrame(inercias.items(), columns=[\"K\", \"inercia\"])\n",
    "df_inercias\n",
    "#Convierto en un dataframe las inercias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRAFICO DEL CODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data = df_inercias, x = \"K\", y = \"inercia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El silhouette es quien verificaba que la selección de grupos sea la correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2 \n",
    "# El 2 es la opcion optima de k elegida gracias al grafico del codo\n",
    "kmeans = KMeans(n_clusters = k, init=\"random\", n_init=10, max_iter=300, random_state=123457)\n",
    "kmeans.fit(datos_escalado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.inertia_)\n",
    "# Da la incercia total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grafico del clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colores = [\"red\", \"green\"]\n",
    "g = sns.scatterplot(x = datos_escalado[:,2], y = datos_escalado[:, 0], palette = colores, hue = kmeans.labels_, alpha = 0.5)\n",
    "g = sns.scatterplot(x = kmeans.cluster_centers_[:,2], y = kmeans.cluster_centers_[:,0], zorder = 10, palette = colores, hue=[0,1], legend = False, marker=6, s=200)\n",
    "#Para ver que pongo en los corchetes, voy a ver lo que me devuelve COLUMNAS y elijo la posicion en base a lo que yo quiero evaluar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg = silhouette_score(datos_escaleado, kmeans.labels_)\n",
    "sample_silhouette_values = silhouette_samples(datos_escaleado, kmeans.labels_)\n",
    "\n",
    "def graficarSilhouette (k, labels, sample_silhouette_values, silhouette_avg):\n",
    "  fig, ax1 = plt.subplots(1, 1)\n",
    "  y_lower = 10\n",
    "  for i in range(k):\n",
    "      ith_cluster_silhouette_values = \\\n",
    "          sample_silhouette_values[labels == i]\n",
    "\n",
    "      ith_cluster_silhouette_values.sort()\n",
    "\n",
    "      size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "      y_upper = y_lower + size_cluster_i\n",
    "\n",
    "      color = cm.nipy_spectral(float(i) / k)\n",
    "      ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "      ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "      y_lower = y_upper + 10\n",
    "\n",
    "  ax1.set_title(\"Plot del silhouette de cada cluster\")\n",
    "  ax1.set_xlabel(\"Coeficiente de silhouette\")\n",
    "  ax1.set_ylabel(\"Etiqueta del cluster\")\n",
    "  ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "  ax1.set_yticks([])\n",
    "\n",
    "graficarSilhouette (k, kmeans.labels_, sample_silhouette_values, silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me dice si la división en grupos que realicé es correcta o no. Mientras más cercano a 1 mejor agrupamiento.\n",
    "\n",
    "\n",
    "Valores negativos: hay datos agrupados en el grupo 0 que no deberían estar ahí."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
